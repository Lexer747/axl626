\documentclass[11pt]{article}

%------------ PACKAGES ----------
\usepackage{mathtools}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{float}
\graphicspath{ {./figures/} {./../Results/} }
\newcommand{\bigO}{\mathcal{O}}
\renewcommand*{\thefootnote}{(\arabic{footnote})}

%------------ MARGINS ----------
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}
\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

%------------ START DOCUMENT ----------
\begin{document}

%------------ TITLE INFO ------------
\begin{titlingpage}
    \centering\par
    {\huge University of Birmingham\par}
    {\small School of Computer Science\par}
        \vspace{0.9cm}
    {\Large Final Year Project\par}
        \vspace{0.9cm}
    {\Huge\bfseries Portfolio Optimization Using Geometric Mean, Risk Correlation 
        \& Multi-Objective Genetic Algorithms\par}
        \vspace{3cm}
    {\Large Alex Lewis\par}
        \vspace{0.9cm}
    {\tiny Supervised by\par}
    {\Large Shan He\par}
        \vspace{0.9cm}
    {\large \today \par}
        \vspace{0.5cm}
        \hrulefill
        \vspace{0.5cm}
    \begin{abstract}
            \vspace{0.1cm}
            An exploration of portfolio optimization theory with a classical geometric
            mean objective, and a novel risk correlation technique to create a secondary
            objective. Thus creating a multi-objective problem which can be applied to
            latest iteration of genetic algorithms - NSGA-II, this more computer science
            based approach tackles portfolio optimization from a new angle. Using the
            implementation documented in this project, the first steps of testing the
            viability of the solutions are then considered. And the space is opened
            up for future real world tests and possible extensions to the risk
            correlation technique. The results achieved in this project are not
            robust enough to replace any existing solutions, and with further testing
            could prove to be a viable alternative to the current options.
    \end{abstract}
\end{titlingpage}

\tableofcontents
\pagebreak

\section{Introduction To Portfolio Optimization}

    A portfolio is a collection of economic assets, and the optimization of such an
    entity is the process of maximizing its return over time, while minimizing the
    risk of the collection.

    TODO: Why is it important

    More specifically, in the case of this project, the assets we are analyzing are
    stocks. Whereas in the more general case, a portfolio would consist of choosing
    assets over multiple markets of completely different types. Such as: the price of 
    materials, currencies, bonds, etc. 

    A lot of work is already based on this simple premise of min-maxing risk and
    reward, it was first discussed by Harry Markowitz in 1952 \cite{Markowitz}
    in his famous paper. He talks about the geometry of the Risk Reward space
    of portfolio's in an abstract way. This was developed further into more specific
    forms: Value at Risk \cite{ValueAtRisk, Ghaoui}, Mean Variance Model (named
    after Markowitz's paper) \cite{Robert, SidWard} and many many more.
    The full review of all these other optimization solutions is best left
    to economists.

    The focus of this project is to use the incredibly hard computable nature
    of this problem, and solve it with insight from computer science. The focus
    is not to fully solve portfolio optimization, but rather taking the problems
    presented in these various papers - in our case, geometric mean \& risk
    correlation - then applying them to the rich data set provided
    by my supervisor, and formulating an experiment to test whether using
    multi-objective genetic algorithms come to good solutions.

\subsection{Portfolio Optimization \& Assumptions}

    Kelly Criterion \cite{Kelly} is a great theoretical achievement, such a simple formula so
    perfectly describes how to play a system in order to gain the most from it. In Fact
    it completely solves how to invest into a single stock. But alas
    stocks and trades are themselves not a repeatable and positive mathematical expectation
    game. Hence why we need to adapt the equations so that we can try to apply them to the
    real world. As well as dealing with more than one stock.

    A big assumption which is being made from now on, by other papers about portfolio optimization - sometimes implicitly,
    is that we can model a stock as a random variable. This is a large assumption. It
    implies, from past data, we can construct a model which will be able
    to predict the probabilities associated with different outcomes of future events.
    This differs from a normal random variable in a few key ways. A random variable in
    mathematics once defined will always act the same. Whereas a stock will
    depend upon its past data to define its functionality, and hence is not fixed to act
    the same.

    This furthermore means that as we gain new data, our model is constantly shifting.
    We do not have a fixed way to tell us probabilities of all future outcomes.
    To top this, we made another assumption: That its feasible to use past data to predict
    future events.

    This line of thinking has been criticized by the famous book "A Random Walk Down
    Wall Street" by Burton Malkiel \cite{BurtonMalkiel} it uses
    key economic historical events and many studies to describe how investing and trading are
    a futile game, compared to compound interest, or using an Index like S\&P 500. Which
    simply is a summation of the top 500 stocks in the US stock market.

    Instead of applying mathematics and statistics to your stock purchasing, the book
    suggests that this is contrary to the 
    very nature of randomness. Humans look for patterns in all things. True 
    randomness seems to be against the very nature of what it is to be human, which is why
    there is still contention as to the ideas in the book. The book reaches a conclusion that
    if you examine stock prices in a given any period you can always find an arbitrary
    system that would produce a positive return for that period.

    A system which will always work and produce a positive return, and be mathematically sound.
    i.e. it will work infinitely. Is an impossibility. We do not have an infinite amount of
    time to test our systems on infinite data.

    However, as much as I like the argument presented in the book and the complete contrast to
    statistically minded way to analyze stocks. Ultimately a stock is buying a piece of an
    actual business something which is valued on how it provides a product or service.
    Looking back at businesses it becomes easier to see how the value of its stock is directly
    related to the people in it, the manning of the ship.

    For me, the only way to reason to myself that statistics, and analysis are still 
    useful. Follows this train of thought: A market is a system of people.
    To predict this system accurately would require knowledge of every individual's
    thought process. Once the system can have accurate predictions made about it's
    future, it is possible to perfectly predict the outcome of every stock. Then
    we simply invest in the highest performing stock - at zero risk. However, this
    dream scenario seems so improbable, that we have to try a different approach
    all together
    \footnote{A system of people is something I don't believe we will ever fully understand.
    Since it would require us to have full understanding of how the brain functions. Which
    is almost certainly beyond the scope of this project}.

    Instead of looking at the causes of stock value. i.e. the people. We can try to look
    at the just the value of the stock and infer from that which stocks are more
    likely to continue similar behaviour. This 
    Statistically and short term analysis can give us insights as to 
    how it might work. Rather than determining the cause and effect. We just use the effect
    to try and predict future effects.

    One does not need to understand gravity to predict a river running downhill. Merely
    enough exposure to any kind of river will show the same effect. To the point where
    the cause of rivers running downhill is unimportant to the prediction that all
    rivers run downhill. Obviously this metaphor is not perfect, but it is in the
    same vein as portfolio optimization. Understanding the causes of stock prices would
    be far more helpful. As understanding gravity confirms that you will \textit{never}
    see a river running uphill. Whereas there will always be some doubt without that
    understanding, that you may find the elusive river which runs uphill.

\subsection{Optimal \(f\)}

    \begin{displayquote}[Ralph Vince \cite{Ralph}] \textit {
        For any given independent trials situation where you have an edge (i.e. positive 
        expectation) there exists an optimal fixed
        fraction (f) between 0 and 1 as a divisor of your biggest loss to bet on each event.
    } \end{displayquote}

    As it turns out this optimal \(f\) is similar to what Kelly describes in his paper. 
    And as Ralph proceeds, he explains how Kelly criterion is a perfect solution for 
    fixed size gambles with fixed wins and losses.

    But then the book reaches the a different conclusion, it goes on to state that trades where 
    the win or loss is always changing {(like the stock market)} then Kelly formula does find
    the correct optimal \(f\).
    So instead the book proposes finding the optimal \(f\) by instead using the geometric
    mean. The geometric mean models the reinvestment technique over a series of trades.
    The most optimal system to reinvest into will have a higher geometric mean than a
    worse system. \emph{Aside: We can use the estimated geometric mean because it is
    basically the same, while being much less computation}:

    \begin{equation}\label{eq:TWR}
        TWR = \displaystyle\prod^{N}_{i=1}1 + f \times \frac{- trade_i}{biggest loss}
    \end{equation}
    \begin{equation}\label{eq:GeoMean}
        Geometric Mean = exp(\frac{1}{N} \times Ln(TWR))
    \end{equation}

    where:
    \begin{flalign*}
    f &= \text{The optimal } f &&\\
    N &= \text{The number of trades} &&\\
    trade_i &= \text{The profit loss of the } i^{th} \text{ trade from the set of trades} &&\\
    biggest loss &= \text{The biggest loss from the set of trades} &&\\
    exp(x) &= \text{The exponential function} &&\\
    Ln(x) &= \text{The natural logarithm function} &&
    \end{flalign*}

    Equation from Ralph Vince's book\cite{Ralph}.
    To maximize our profit from a set of trades we want to optimize for the highest possible 
    geometric mean. Since \(f\) is a free standing variable which cannot be made the subject 
    of the equation we can only use iteration to find a good estimate. Or genetic algorithms
    as explained later.


\subsection{Portfolio's Compared to Single Stocks}

    Its a classical way of thinking to also consider diversification
    of your investments. Intuition tells us a single stock with a calculated optimal \(f\) that predicts
    good results, may still have "unlucky streaks" in which you can run out of money before
    your luck turns. This concept is intuitive, \textit{"Don't put all your eggs in
    one basket"}. Otherwise our problem would be solved, by using Kelly's
    formula \cite{Kelly} we find the return of each stock individually, choose
    the highest stock. Our portfolio is simply that stock.

    \begin{equation}\label{eq:Kelly}
        f = \frac{(b \times p - (1 - p))}{b}
    \end{equation}

    where:
    \begin{flalign*}
        f &= \text{The best size as a fraction of your portfolio} &&\\
        b &= \text{The net odds on the wager. i.e. your win } b + \text{ wager staked} &&\\
        p &= \text{The probability of winning} &&
    \end{flalign*}

    Therefore we want a way to reduce our \(f\) by the correct amount
    to account for large streaks. Additionally we use the left over unused money to invest in
    another (ideally uncorrelated) stocks.
    This complicates the problem of finding optimal \(f\)'s for a set of stocks. As we
    need to find a set of \(f\)'s for each stock, to best diversify our portfolio, so
    its most resistant to unlucky streaks. While still producing good geometric growth.

    Thinking of the optimal \(f\) as a single point, inside a space of trades/bets. When we have 
    only a single trade/bet, we are doing calculations in a 2D space, and we have a line which 
    represents out optimal \(f\). However as the number of trades/bets increases we gain
    another dimension for each one.

    Therefore instead a \(f\) which lies on a line, it lies on a plane, or a 3D object, 
    and etc. If the \(f\) suddenly becomes a multi variable coordinate which must be 
    exactly correct. If a single axis is out then you can miss the hill of positive 
    growth, even if every other axis lined up.

    This means we need to define a new function to find the individual \(f\)'s for all the bets,
    but with relation to each other. We cannot compute each optimal \(f\) for every stock
    independently.


\subsection{Risk - A New Interpretation}

    Linking back to Kelly Criterion, the riskiness of a random variable is accounted for
    when use Kelly's formula, equation \ref{eq:Kelly} needs no further calculations
    to account for how risky the variable is. And this is also true of geometric mean growth
    for stocks. The geometric accounts for the variance in the data of a single stock. As the
    equations \ref{eq:TWR} \& \ref{eq:GeoMean} are accounting for risk implicitly.

    But we need a new calculation to find the risk of the portfolio. And furthermore we would
    like a way to correlate stocks together so that we can more wisely spread our money.

\subsubsection{Mathematical Definition}

    Given the motivation from our the above sections, we would now like to formally define our
    evaluation function for how well a portfolio performs given a set of \(f\)'s. Which describes
    how we spread our money over a set of stocks.

    \begin{equation}\label{eq:G}
        G(f_1...f_m) = \left( \displaystyle\prod^{m}_{k=1} HPR_k \right) ^{ \left( \displaystyle\frac{1}{\sum^{m}_{k=1}Prob_k} \right)}
    \end{equation}
    \begin{equation}\label{eq:HPR_k}
        HPR_k = \left( 1 +  \displaystyle\sum^{n}_{i=1} f_k \times \frac{- PL_{k,i}}{BL_k} \right) ^{Prob_k}
    \end{equation}
    \begin{equation}\label{eq:Prob_k}
        Prob_k = \left( \displaystyle\prod^{n - 1}_{i=1} P(i_k | j_k)\right)^{\frac{1}{n - 1}}
    \end{equation}

    where:
    \begin{flalign*}
    n &= \text{The number of trades or bets in the } k^{th}\text{set} &&\\
    m &= \text{The number of stocks} &&\\
    f_k &= \text{The optimal } f \text{ for that }k^{th} \text{ set, where } f > 0 &&\\
    PL_{k,i} &= \text{The outcome for the } i^{th} \text{ trade or bet associated with the } 
        k^{th} \text{ set} &&\\
    BL_k &= \text{The worst trade or bet for the } k^{th} \text{ set} &&\\
    P(i_k | j_k) &= \text{Roughly it is the risk of }i^{th} 
        \text{ trade or bet associated with the } k^{th} \text{ set given the risk of the } &&\\
        & j^{th} \text{ trade or bet associated with the } k^{th} 
        \text{ set. Which is easy to calculate for coins, but I will explain} &&\\
        & \text{later how it is calculated for actual stocks} &&
    \end{flalign*}

    This equation is also from Ralph's Book \cite{Ralph}. Which is an excellent starting point,
    but it needs to be adapted to include this idea of correlation among stocks. And furthermore
    it needs to be decoupled, so that the risk of the overall portfolio is also found. The motivation
    for such a thing, is because we would like to be able to create a range of  portfolio's with
    differing levels of risk and reward. Allowing a choice inside this range.


\subsubsection{Calculating Risk} \label{section:CalcR}

    Here we are actually constructing the risk calculation for a stock in a more concrete form.
    So must operate under some assumptions and natural patterns of a stock. 

    Proposed Model of a Stock: 
    \begin{itemize}
        \item{Open {\&} Close values for a given arbitrary time period}
        \item{A ratio of correlation to other stocks}
        \item{Holes of Open {\&} Close values may occur}
    \end{itemize}

    Calculating the profit/loss of a stock at time period is now trivial and useful.
    Renaming the change in value at each time period \(i\) to \(PL_i\) we can find
    statistical facts about the stock.

    \begin{align}
        \text{mean: }
            \mu &= \frac{\sum^{i}_{n=1} PL_n}{i} \label{eq:StockMean} \\
        \text{variance: } 
            \sigma^2 &= \frac{\sum^{i}_{n=1} (PL_n - \bar{PL})^2}{i} \label{eq:StockVar}
    \end{align}

    where:
    \begin{flalign*}
    i &= \text{The number of trades for a given time period in the stock } PL &&\\
    PL_n  &= \text{The result of trading the stock on the } n^{th} \text{ trade} &&\\
    \bar{PL} &= \text{The mean of the stock } PL \text{ as calculated by equation \ref{eq:StockMean}} \\
    \end{flalign*}


    Using this data we can transform a stock into a distribution which we can use to estimate
    the likely hood of the next \(PL_i\) being above a certain value. In our case we transform
    the normal distribution using equations \ref{eq:StockMean} and \ref{eq:StockVar} to find
    the mean and variance of the stock, and hence change the normal distribution. Then it is
    simple enough to use tables to find probabilities we want. It should be noted that the
    assumption that a stock is a normal distribution has sizable implications, it ties into
    our original assumption that stocks can be modeled as random variables.

    \begin{equation} \label{eq:StockProb}
        P (PL) = \big( PL \sim \Phi(\mu, \sigma^2) \big) < 0
    \end{equation}
    
    where:
    \begin{flalign*}
    PL \sim \Phi (\mu, \sigma^2) &= \text{The cumulative normal distribution, adjusted to the mean } \mu \text{ and variance } \sigma^2 &&\\
    \text{ of the stock } PL\\
    \end{flalign*}

    Since calculating \(\Phi (\mu, \sigma^2)\) is incredibly hard to calculate and
    a perfect value isn't critical, we can settle for using tables as a close enough
    estimate.

    Accounting for 'holes' in the data, and the fact that older data may not be as relevant. Is
    currently overlooked if we were just going to implement these functions as our risk. The
    first change I am going to make is that we specify a time period and a range of time to
    take data from. This has multiple advantages, the first one being that we can choose to
    use only the most recent data about a stock. The second being that we can also choose
    how granular to make our risk calculation. It also builds directly into the algorithm
    the fact that the calculation will not use all available data from a stock, hence dealing
    with 'holes' at the same time.

    Now for the more complex part of the risk calculation, while the algorithm for it is
    simple, finding the data for it is a hard problem in itself. The equation
    \(P(i_k | j_k)\) hides the details behind the \(|\) symbol. Normally in statistics
    and probability of \(P(X | Y)\) means 'The probability of \(X\) given \(Y\) has occurred'
    which in a normal probability space is actually just a shorthand for:

    \begin{equation*}
        P ( X | Y ) = \frac{P(X \cap Y)}{P(Y)}
    \end{equation*}

    But in our stock based world we don't have an equivalent function \(\cap\) and hence
    we don't have the function \(|\) as well. The best we can do is assume that \(\cap\)
    is roughly equivalent to multiplying together the two stocks in question, and then
    assuming that \(\div\) is roughly equivalent to multiplying that by some amount of
    correlation. We cannot just multiply and divide as if they are independent probabilities as
    the first stock will just cancel out, hence doing nothing. So our function becomes:

    \begin{equation*}
        P ( X | Y ) = P ( X ) \times P ( Y ) \times C_{X, Y}
    \end{equation*}

    where:
    \begin{flalign*}
    C_{X, Y} &= \text{The pre-calculated correlation of stock } X \text{ to stock } Y&&\\
    P( X ) &= \text{The probability of } X \text{ given by the equation \ref{eq:StockProb}} \\
    \end{flalign*}

    However this doesn't work for lots of stocks as each probability is a smaller value 1, so
    when we times them all together, the value gets ever smaller regardless if each chance was
    actually quite high. For example, say we had 25 stocks, and each stock has a 0.9
    chance to make money, and for simplicity each correlation to every stock is 0.8:

    \begin{flalign*}
        \text{for all stocks } P &= 0.9 \times 0.9 \times 0.8 &&\\
        \text{hence } P &= 0.648 ^ {25} &&\\
        P &= 0.00001947041 \\
    \end{flalign*}

    So even in this example, with really high odds, the overall chance will be incredibly low.
    Therefore we need to use something which both preserves
    the chances of every stock, while taking into account how correlated something is.

    The solution is to use a weighted average over the stocks.
    And in the case where the correlation is negative we simply do \(1 - P(PL)\)
    when we calculate the average.

    \begin{equation} \label{eq:StockWeight}
        P ( X | Y ) = 
        \begin{cases}
            \displaystyle\frac 
                {P( X ) + (P ( Y ) \times C_{X, Y})}
                {1 + C_{X, Y}} 
                & \text{ if } C_{X, Y} >= 0\\
            \\
            \displaystyle\frac
                {P( X ) + ((1 - P ( Y )) \times - C_{X, Y})}
                {1 - C_{X, Y}} 
                & \text{ if } C_{X, Y} < 0
        \end{cases}
    \end{equation}

    Which brings us back to the claim that the algorithm is simple - all we do is find a
    weighted average of the stocks based on how correlated they are but, how do we find 
    and calculate good values for the table \(C\)? Some solutions are:

    \begin{enumerate}
        \item{Calculating the actual correlation, either Spearman's, or some other mathematical technique}
        \item{Using some form of NLP (Natural Language Parsing) to figure out how related the stocks are in the real world}
        \item{Grouping stocks by which industry they are in and assigning each group a correlation}
        \item{Assuming all stocks are independent}
        \item{Some other way of correlating stocks I haven't considered}
        \item{Or a combination of all the above}\label{item:C}
    \end{enumerate}

    The answer to the question: how to find a good \(C\)? is probably worth its own paper, since
    its almost definitely point \ref{item:C}. Which requires a more complex analysis of the systems
    that make up a stock. At the end of the day the numbers we can gather about a stock never
    tell the full story. A stock is a part of a business, and a business is simply a is a
    group of people working together to sell a product or service to other people.
    And at the end of day using numbers to analyze how people will succeed in selling
    a product or service will always come up short compared to knowing and understanding
    the people who are ultimately doing the selling.

    For the sake of simplicity we will calculate the correlation as its mathematically defined.
    For example calculating the correlation between 2 stocks \(PL_1\) and \(PL_2\) is
    done as so:

    \begin{align}
        C_{PL_1, PL_2} = 
        \frac{
            \displaystyle\sum^{n}_{i=1} (PL_{1, i} - \bar {PL_1})(PL_{2, i} - \bar {PL_2})
        }{
            \sqrt{
                \displaystyle\sum^{n}_{i=1}(PL_{1,i} - \bar {PL_1})^2 
                \displaystyle\sum^{n}_{i=1}(PL_{2,i} - \bar {PL_2})^2
            }
        }
        \label{eq:Correlation}
    \end{align}

\section{Decoupling \(G\)}

    In the original equations \ref{eq:G}, \ref{eq:HPR_k}, and \ref{eq:Prob_k}.
    \(G\) is coupled with the risk of the portfolio
    by raising the inner calculation to 1 over the sum of all risks for every stock.
    There is another approach, which is to keep the two equations separate and use multi-
    objective genetic algorithms to solve it. The motivation for this is to balance risk
    and reward independently, contrasting to the original \(G\) which lowers the gain
    by coupling it with the risk exponentially. We would also gain a Pareto set of
    optimal portfolios to choose from, if one was more risk averse then you could choose
    a less risky portfolio from the set.

    \begin{equation}\label{eq:DecoupleG}
        G(f_1...f_n) = \displaystyle\prod^{m}_{k=1} \left(
                1 + \displaystyle\sum^{n}_{i=1} f_k \times \Big(
                    \frac{- PL_{k,i} }{BL_k}
                \Big)
            \right)
    \end{equation}

    \begin{equation}\label{eq:DecoupleR}
        R(f_1...f_n) = \displaystyle\prod^{m}_{k=1} \left(
                1 + f_k \times \Big(
                    \forall y \in m \to P(k|y)
                \Big)
            \right)
    \end{equation}

    where:
    \begin{flalign*}
    n &= \text{The number of trades or bets in the } k^{th}\text{ set} &&\\
    m &= \text{The number of stocks} &&\\
    f_k &= \text{The optimal } f \text{ for that } k^{th} \text{ set, where } f > 0 &&\\
    PL_{k,i} &= \text{The outcome for the } i^{th} 
        \text{ trade or bet associated with the } k^{th} \text{ set} &&\\
    BL_k &= \text{The worst trade or bet for the } k^{th} \text{ set} &&\\
    P(k|y) &= \text{Equation \ref{eq:StockWeight} with the stock } k \text{ correlated 
    with every other stock in the set } m &&
    \end{flalign*}

\subsection{Inspiration and Novelty?}

    Now we can use these functions separately to evaluate both risk and reward of the
    portfolio. You get a Pareto Set \cite{Kaisa}. A non dominated efficient frontier. Fortunately
    this is not a "problem" in some sense, as a choice of risk vs reward can then be made
    by a human. To the best of my current knowledge equation \ref{eq:DecoupleR} is a new
    way of thinking about the risk of the portfolio.
    Section \ref{section:CalcR} reaches a conclusion which I believe to be
    unique. However, Ralph's work \cite{Ralph} is what inspired this line of thought
    and furthermore it is the basis of my work for equation \ref{eq:DecoupleR}, as well as
    the aggregate for equation \ref{eq:DecoupleG}.

    TODO: Summarize main contributions

\section{Solving \(f\) with respect to \(G\) and \(R\)}

    Solving the decoupled equations \ref{eq:DecoupleG}, \ref{eq:DecoupleR} for the highest
    value of \(G\) and lowest value of \(R\) will give the best return on our portfolio
    given the current data.
    Intuition would suggest some form of iteration to check every combination of \(f\) to find
    the best performing \(f\) is a good first solution. This has two major problems.
    The first problem with this solution. For every stock we add to our portfolio, 
    we gain fractorially many more combinations to check.

    \begin{equation*}
        \text{Number of combinations to check for: } (f_1...f_m) = m!
    \end{equation*}

    Which in some scenarios may be feasible, but because of our second major problem. The computation
    time of our evaluation functions. It becomes completely impossible to actually calculate
    the mathematically correct solution in the lifetime of the universe.
    What follows is the Big \(\bigO\) calculation:

    \begin{equation*}
        \text{Decoupled: } \bigO (
            m! \times (n^m + (nm)^m)
        )
    \end{equation*}

    where:
    \begin{flalign*}
    n &= \text{The number of trades or bets in the stock} &&\\
    m &= \text{The number of stocks} &&
    \end{flalign*}

    The decoupled equation grow
    in number of calculations incredibly quickly, in fact too quickly to ever be
    solved by a pure mathematical solution. So we end up with an equation we want
    to solve, with no mathematically helpful properties (differentiable being
    the most important) to be able to do a heuristic or regression based approach.


\subsection{Genetic Algorithms}\label{section:GA}

    \begin{displayquote}[Albert Bethke \cite{Bethke}] \textit {
        To find the point at which a real-valued function takes it maximal value ...
        If you don't know of any special properties of the function, then you
        should use a method which is more general like genetic algorithms.
    } \end{displayquote}

    Genetic algorithms (GA), first 
    discussed by John Holland \cite{Holland} in 1975, then developed by so many
    others, such as Melanie Mitchell, Kalyanmoy Deb, and Albert Bethke
    \cite{Mitchell, KalyanmoyDeb, Bethke}, to name a mere few. Are inspired by Darwinian
    principle of \textit{survival of the fittest}.
    First it is useful to define some biological terminology; in the context of
    genetic algorithms these terms are used in the same spirit as the real biology,
    but heavily simplified. In an attempt to emulate their nature as a living entity,
    while being computationally easier to process.

    \begin{itemize}
        \item{\textbf{Gene:} The encoding of a trait, and roughly it stores the current
            setting for the trait (e.g. eye colour, blue, brown, etc). There is a set
            from which any single gene can pull its data from. In nature its the chemical
            bases  of DNA}
        \item{\textbf{Genome:} The complete collection of all the genes is called a
            genome}
        \item{\textbf{Reproduction:} During reproduction each parent's genomes will be
            combined and crossed over creating a child which has a combination of
            the traits of its parents}
        \item{\textbf{Mutation:} During crossover there is a chance a single gene
            may be incorrectly copied, and mutate into a different gene}
        \item{\textbf{Fitness: } In nature the fitness of an animal determines
            its chances of survival and therefore its chance to pass on its genes
            and be the most successful animal. In GA the level of fitness is not assessed
            by nature, but by an \textbf{Objective} function}
    \end{itemize}

    The general procedure of a GA is better explained with a diagram:

    \begin{figure}[H] % fig:GA
        \centering
        \includegraphics[width=\textwidth]{GA}
        \caption{"Life Cycle" of A Genetic Algorithm}\label{fig:GA}
    \end{figure}

    One note about figure \ref{fig:GA} is that the decision to go to the "Stop"
    state is determined by some outside factor i.e. execution time. Otherwise
    it repeats infinitely - just like real life. Applying this generic algorithm to 
    our specific problem. We simply need to
    define each state in this diagram in the context of our problem.

    The \textit{Genomes} state for our problem is the set of \(f\)'s. Each gene is an
    \(f\) for a specific stock. But we have additional constraints, which is for 
    any genome \((f_1...f_m)\) made up of genes \(f_i\). The genome \textbf{must}
    abide by these laws:

    \begin{align*}
        \text{Law: } & \;
        0 < \left(
            \displaystyle\sum^{m}_{i=1} f_i
        \right) <= 1 \\
        \text{Law: } & \;
        \forall f_i \in (f_1...f_m) \to \left(
            0 < f_i <= 1
        \right) \\
    \end{align*}

    which more simply states, we can't bet more than our entire wealth over all the stocks or
    a negative bet. And on a single stock we can't place a negative bet more than our
    entire portfolio.

    The \textit{Select} state is a function which takes all pairs of evaluated genomes
    and their fitness. And based on how well each genome performs, will cull the
    list of genomes to a smaller list. This smaller list is the list of genomes
    which act as parents for the next generation.

    The \textit{Elite} state is another output from the select function. Which is
    the most fit genomes have their lifespan extended into the next generation.
    Importantly this elite population be very small, as well as avoiding the
    chance to its genes to be destroyed by crossover and mutation. \cite{DeJong}
    This allows the best genome to always stay alive, and stops the GA from
    having to re-discover previously good results.

    The \textit{Crossover} state can simply be the same as any generic GA implementation,
    as we have no extra constraints to worry about. In the actual implementation
    a single point crossover, of high chance of occurrence was chosen. This was
    chosen arbitrarily, it could be worth a in-depth study to which crossover
    method would best perform for this specific problem. Since there is no catch all
    "best" crossover method.

    The \textit{Mutate} state needs to make sure it follows the same constraints as
    genomes states has to follow. Furthermore since this step is after crossover,
    any genomes which violate the laws will be fixed in this step. Other than this,
    mutation can be any generic GA mutation. The specific mutation chosen was a
    simple Normal distributed mutation, which is randomly added or subtracted from
    the original gene. Similar to the specific crossover choice, this choice
    of mutation is probably worth its own in-depth study to optimize how fast
    the GA finds the solution. As there is also no catch all "best" mutation method.

    Finally the \textit{Calculate Objectives} state, which is the final product of all our
    mathematical exploration. Here we have two options the single variable option: equations
    \ref{eq:G}, \ref{eq:HPR_k}, and \ref{eq:Prob_k}. Or the decoupled multi-objective
    option: equations \ref{eq:DecoupleG} and \ref{eq:DecoupleR}. 

    Other than the mathematical difference between single and multi-objective,
    another key difference between single objective and multi-objective is that for a
    nontrivial problem such as ours, no single solution exists that simultaneously
    optimizes each objective. This case of conflicting objectives means there is
    a (possibly infinite) number of Pareto optimal solutions, forming a Pareto front.
    The problem of sorting a set of genomes is complex problem, and furthermore
    does not remove problem of fronts forming rather than complete solutions.
    The way the results of our GA are sorted are according to the paper headed by
    Kalyanmoy Deb \cite{DebPratapAgarwalMeyarivan} in which they propose the
    NSGA-II, the cutting edge of multi-objective solution finding. This actually
    has a direct impact on the selection step of the GA.

\subsection{NSGA-II vs Other Multi-Objective Algorithms}

    As there exists multiple Pareto optimal solutions for our problem, what it means to "solve"
    our problem, is not as simple as single objective problems. We actually want to create the
    Pareto set of optimal solutions and then to form a single solution we have to choose one of
    the Pareto solutions from that set.

    There are two methodologies to go about this selection of a single solution. A \textit{priori}
    methodology, in which the Pareto set is constrained by some rules provided by a user.
    These rules result in a single solution being used each step of GA. The other
    methodology is the \textit{posteriori} methodology, in which
    the full set of Pareto optimal solutions is kept and a user must decide which individual
    solution is kept only after the GA has finished execution.

    Linear scalarizing \& Non-linear scalarizing are two priori ways to solve this problem
    \cite{KaisaMarko, Moffaert}. Both of the cited papers give a range of linear and non-linear
    functions which can be used either with weightings by the user, or utility functions.
    To list a few: The simplest being a weighted average of all the objectives, from
    which a single objective if found. Another version is a single point is chosen
    in the answer space, then the distance to that point is calculated and used to
    determine how good a solution is. A lower distance will mean its a better solution
    \cite{Buchanan}.

    To use utility functions as scalarizing solvers, this utility function will take
    the resulting objectives and return and ordering of ranking. Upon which the highest
    ranked is considered the best solution. This requires the user to construct
    a function which accurately represents the interests of user. A simple way
    to do this is to ask the user to list the objectives in a lexicographical order.
    But in practice a utility function is very hard to create, and the simple
    lexicographical solution is not well suited to our needs. As it requires
    strictly preferable objectives.

    To name some posteriori methods, there's Normal Boundary Intersection \cite{Indraneel},
    Normal Constraint \cite{Messac}, Directed Search Domain \cite{Tohid}, and NSGA-II
    \cite{DebPratapAgarwalMeyarivan}. All except NSGA-II work by constructing several
    scalarizations, with each scalarization creating a Pareto optimal solution.
    This also leads those algorithms to end up in local maximum far to often. And
    don't get the full efficient frontier.

    However NSGA-2 is different to the previous ones, and for the reason to be explained,
    and because its specifically useful for our problem, this is the algorithm that
    we use to find the Pareto frontier. To explain NSGA-2, first we need to explain
    NSGA, and non-domination.

    \textbf{Domination}: A solution \(x\) is said to dominate another solution \(y\) if:

    \begin{align*}
        & x \text{ is no worse than } y \text{ for all objectives} \\
        & x \text{ is strictly better than } y \text{ in at least one objective} \\
    \end{align*}

    This is denoted in most papers as \(x \prec y\), to further this: Among a set of solutions \(P\)
    the non dominated sub set \(P'\) are those that are not dominated by any member of
    set \(P\). To follow on from this the a non-dominated set of the multi-objective search
    space, will be the global Pareto optimal set.

    NSGA-II has the following procedure: perform a non-dominated sorting in the population
    and classify them by front. i.e. sorted according ascending level of non-domination.

    \begin{figure}[H]
        % \centering
        \includegraphics{NSGArank}
    \end{figure}
    From this example graph, we have split the data up into 3 sets which are non-dominated
    by each other. The set contained by the blue and first line is a non dominated sub set
    of the set between the two lines. The set between the two lines is also a non dominated
    sub set of the final set. This allows us to assign a rank to each band.

    The next step is to make the next generation of parents from the front ranking
    set, as well as some random amount of the inferior bands. This population
    then goes under the next steps in GA and the cycle repeated.

    This method of multi-objective has some big advantages compared to the other methods,
    the most important being that the pareto set is collected instead by using the non-dominated
    sorting. While the solutions aren't guaranteed to be optimal pareto solutions, they
    get very close. Furthermore this method was designed with GA's in mind and therefore is
    optimized for this usage. Hence why I'm choosing to use NSGA-II in my solution.


\section{Implementation Details \& The Experiment}

    Now that we have problem, and a method to solve it, all thats left is to actually
    implement it. Then test it on some data, comparing our hypothesis to the results.
    I choose to implement the system using Haskell. This is definitely a personal
    choice, for the standard reasons: familiarity, ease of use, having the environments
    set-up, and the strong type system. Because when working with a lot of "pure"
    functions, as we would have to for this project. The implementation 
    of the mathematical part of this project was made significantly
    easier, less buggy, and probably more performant because of the language choice
    \footnote{Being pure makes parallelism a no cost feature \cite{HarrisMarlowJones, Chakravarty},
    which will give great performance improvements due to lightweight nature of the 
    computation, while needing lots of locking. }.
    In my eyes the mathematical functions
    being implemented purely reduces the chance to almost zero for there to be a translation
    error. This was helpful during the development, as any erroneous values implied a mistake
    in the maths.

    Another key choice made, was the use of Moo - a GA library \cite{Moo}. This came with all
    the normal advantages and disadvantages of using a library, time saved, tested by others.
    But it still required a fairly large amount of work to correctly integrate the
    problem into the library. Additionally you do not have complete control over the code,
    so any "mistakes" or differences in behaviour are out of your control. However,
    this meant that once this work was done the actual execution of the GA was more simple.

    The experiment involved the following method. First I choose a portfolio I want to
    analyze, in this case I used the data set provided by my supervisor \cite{Dataset}.
    Obviously there are drawbacks to this as later explained in the results. The next step is
    to have some point of comparison, the obvious choice is to compare the GA to
    a complete naive spread i.e. evenly spread out the starting money over each stock.
    I also choose to compare the GA to the UK national interest rate \cite{BankOfE} . To give an
    indication of how well the algorithm is performing compared to doing "nothing".

\subsection{Implementation Overview}

    The implementation is broken up into four main sections:

    \begin{itemize}
        \item{Dataset Parsing}
        \item{Calculating Correlations \& Risk}
        \item{Calculating Objectives}
        \item{Executing the GA}
    \end{itemize}

    \textit{Dataset Parsing} was implemented in a concrete form to directly work with the
    dataset I will use in the experiment. This means the code reads the CSV files,
    performs the necessary processing to convert them into the Haskell types, and then
    basic statistical analysis to extract trades.

    \textit{Calculating Correlations} is the implementation of the equation \ref{eq:Correlation}.
    Additionally we need to correlate every stock to every other stock. Because equation
    \ref{eq:DecoupleR} requires every stock included in the risk calculation has some value of
    correlation for all other stocks.

    Therefore in the worst case we need to do \(m!\) correlation calculations, each one requiring a
    full traversal of all the trades in the stock. This gives a full time complexity of 
    \(\bigO (nm!) \). This can be speed up a little bit in the actual implementation, firstly
    equation \ref{eq:Correlation} can be done in a single pass instead of two. Secondly we can
    half the number of permutations we need to do the correlation calculation for. Since we know
    correlation is symmetric, i.e. if the correlation of \(C_{x,y} = z\) then the correlation
    of \(C_{y,x}\) also equals \(z\). While this doesn't change the time complexity, it still helps
    the execution time, as we only need half as many calculations. On top of this, and as explained
    earlier, this correlation calculation can be run in parallel using the pure nature of the function.
    All of equation \ref{eq:Correlation} is done in a unique threads using Haskell to abstract away
    the thread management.

    Furthermore, we can calculate the correlations ahead of the GA execution so this only needs to be
    executed once and the results stored. In the implementation the results are stored as a symmetric
    adjacency list with two stocks acting as a key to the value of the correlation between the stocks.
    Hence this bad time complexity isn't a negligible worry.

    \textit{Calculating Risk} is the implementation of the inner equation \ref{eq:DecoupleR} 
    i.e. (\(\forall y \in m \to P(k|y)\)). This equation
    requires the correlation list to be calculated, because of equation \ref{eq:Correlation} being a
    part of equation \ref{eq:DecoupleR}. It can also be optimized such that the individual
    probabilities of each stock (calculated by equation \ref{eq:StockProb}) are only needed to be
    calculated once.

    \textit{Calculating Objectives} is the implementation of equation \ref{eq:DecoupleG} and the complete
    implementation of equation \ref{eq:DecoupleR} using the earlier risk calculation, these
    equations both take a list of \(f\)'s as their input to be able to calculate \(G\) and \(R\).
    And once again, this implementation takes advantage of Haskell's low cost parallelism to speed
    up execution.

    \textit{Executing the GA}, as explained earlier the Moo GA Library \cite{Moo} was used in
    the implementation. The additional details added in my solution are the printing of the results,
    the constraints specified by the laws of a portfolio, and the actual programming interface
    between the equations and the library code.

\subsection{Testing Method}

    The general testing method was as follows. Over the dataset, split it into moving windows
    where each window covers a certain time range of data. The windowing is done to emulate
    the process of eventually considering that old enough data will become less useful
    as an indicator of future events. The only piece of data that will be kept regardless
    of window size, is the \(BL\). This is to be more realistic about the worst case scenario.
    As it could be seen as reasonable to say the \(BL\) of any asset is \(\infty\), if the value
    of the asset becomes unsellable, this rather extreme view however essentially means all
    algorithms would tell you to bet nothing. Since thats the superior strategy. Instead
    this approach, takes into account the worst \(BL\) of any one stock over its lifetime.
    Since almost every asset has the potential to fall as far as it once did. Even if we only
    consider is most recent results in the window, its helpful to suspect an asset could
    do that poorly again.

    The next step is to choose the window size and the dataset of assets to be tested upon.
    In my case I used my supervisor's dataset \cite{Dataset} and I elected to choose a window
    size of 3 years.

    After the window size and dataset is chosen the next stage is to run preliminary tests with
    differing lengths of time to find the a reasonable amount of time to run the GA based on
    the hardware. From that you have make a human decision to compromise the amount of time
    you allow the GA to find a solution and how much time you actually have to test the GA.

    Next, you actually run the GA from the start of your dataset over every window and record
    the results. All thats left would be to compare these results the naive spread of the
    portfolio, or any other investment strategy for that matter.

\subsection{Results}

    First the preliminary results of running the GA over 1 window (of 3 years) with different
    times to determine how much time to spend. It is also useful as this graph also shows
    the full pareto frontier.

    \begin{figure}[H] % fig:HowTheParetoFrontierChangesWithGADuration_2017-1Window
        % \centering
        \includegraphics[width=\textwidth]{HowTheParetoFrontierChangesWithGADuration_2017-1Window}
        \caption{Preliminary : Pareto Frontier}
            \label{fig:HowTheParetoFrontierChangesWithGADuration_2017-1Window}
    \end{figure}

    The biggest takeaway from this graph is that it was very hard to improve \(R\) beyond
    the initial solutions. But the gain seemed to constantly improve given exponentially
    more time. There was a significant jump in results from 1 minute to 3 minutes,
    therefore I choose to do all future tests with 3 minutes. Once again this is slightly
    arbitrary, since if wanted the "best" results I would've chosen 20 minutes (1200 seconds).
    But that amount of time for a single run is a bit extreme. Furthermore the sample size for
    each duration is only 1, so 3 minutes was more a estimated timing based loosely
    on this result.

    Next I ran the GA over more windows to see how it performs:

    \begin{figure}[H] % fig:BackTestingTheResultsOfTheGA
        % \centering
        \includegraphics[width=\textwidth]{BackTestingTheResultsOfTheGA}
        \caption{Gain of GA vs Naive vs UK interest}
            \label{fig:BackTestingTheResultsOfTheGA}
    \end{figure}

    To create figure \ref{fig:BackTestingTheResultsOfTheGA} the GA was run the same way as
    figure \ref{fig:HowTheParetoFrontierChangesWithGADuration_2017-1Window}. From the
    frontier of that test, arbitrarily, I then choose the highest \(G\) scoring result.
    This portfolio was then back tested on the exact window it was trained on. This
    means the \(f\)'s were invested as described and the bar on the graph shows
    how return on investment per annum the portfolio had.

    A clear trend is that the GA outperforms the naive spread, very consistently.
    Only in 3 windows does the naive spread do better than any GA run: 2007, 2008, 2017.
    But the naive spread still lost money in 2007 and 2008. Making 2017 an outstanding result,
    as one could word that to say naive was better than the GA 3\% of the time.
    Overall from the two runs of the GA, 68\% of the time the GA turned a profit.
    And only 57\% of the time did the GA actually beat UK national interest.

    However, when the GA did beat out the other options it did so by a large margin. In
    the 2014 window both runs made over 30\% return per year over 3 years! This is also true
    in the inverse. When the GA lost to the other options it did so by a large margin.
    In 2007 window the GA first run lost 30\% and the second run
    lost 75\% of the original investment, per annum! This huge variation is slightly worrying
    and does not lead to good conclusions about the reliability of this method.

    Some of this variation could be explained away as the fact that I elected to choose the
    highest \(G\) scoring portfolio's, which consequently tended to have the highest \(R\)
    score.

    \begin{figure}[H] % fig:GeneralStockDistributionCombinedOverAllWindowsTested % fig:HowTheInduvidualPortfolioWasDistributedOverEachWindow
        \centering
        \includegraphics[width=\textwidth - 1in]{GeneralStockDistributionCombinedOverAllWindowsTested}
        \caption{Selectiveness of the GA}
            \label{fig:GeneralStockDistributionCombinedOverAllWindowsTested}
        \includegraphics[width=\textwidth - 1in]{HowTheInduvidualPortfolioWasDistributedOverEachWindow}
        \caption{Distribution over each window}
            \label{fig:HowTheInduvidualPortfolioWasDistributedOverEachWindow}
    \end{figure}

    This is the results of the GA run 2, and plotting the \(f\)'s from each window as
    a stacked chart. What is obviously apparent here is that the GA definitely favours
    certain stocks, and even more apparently in figure
    \ref{fig:HowTheInduvidualPortfolioWasDistributedOverEachWindow} it does not always
    do a good job at distributing a diverse spread. In figure
    \ref{fig:HowTheInduvidualPortfolioWasDistributedOverEachWindow} in the 1998 window
    you can see how 2 stocks are favored very heavily, AIRI and AKO B. Interestingly
    enough AIRI was not a traded stock till the year 2000, which means that in 1998
    investing in AIRI was the equivalent to not investing at all. As the implementation
    specifies a stock which no data for that window has gain of 0.

    Additionally in the window 2001 in figure
    \ref{fig:HowTheInduvidualPortfolioWasDistributedOverEachWindow} Over 55\% of the portfolio
    is in a single stock AINV, which is not distributed evenly at all. This contrasts
    completely to 2010 where a very good spread is achieved, the high value stocks are found
    and invested in more, but it still had a good spread over the less good stocks to
    increase diversity.

    From figure \ref{fig:GeneralStockDistributionCombinedOverAllWindowsTested} its
    clear that over time a fairly good spread is achieved, but there are some definite
    outliers which are invested in far more than other stocks. AKO A and AKO B make up
    20\% of all investments over all windows. Which is very high compared an even spread
    in which they would take up 10\% of all investments over all windows.

\section{Conclusions}

    From the results the GA performed adequately, its a high risk
    high reward strategy. It could certainly do with improvements and further testing.
    Such as: Forward testing the results; and a different selection of portfolio's
    from the pareto frontier. Additionally more runs of the GA are definitely required
    to get a larger sample size of its real world performance. Then some measure of
    means test to check whether the results are just due to variance,
    before any real conclusions can be draw about the strength of the method. This
    project is still a good first step into using the novel risk correlation
    as a second objective when optimizing portfolios. It makes excellent use
    of the advancements in multi-objective GAs, NSGA-II is the perfect partner.

    Compared to previously described portfolio schemes
    \cite{Markowitz, ValueAtRisk, Ghaoui, Robert, SidWard} this scheme and solution
    is not tested well enough to be considered a viable alternative. Furthermore
    I'm not an economist. This scheme would benefit greatly from some criticism
    from actual economists. As well as real world use alongside the other methods
    for a much better test of the solution. My results also tie back in with
    Random walk mentality, its hard to tell if this solution will ever
    work in the long run.

    In terms of implementation I am pleased with how small the actual code base needed to
    be to implement such a complex system. This is definitely helped by the language choice,
    the inherent parallelism gained, and pure mathematical functions made this
    implementation very compact. There could be definite improvement in the
    program if it were to be a real tool with respect to usability and customization. Figures
    \ref{fig:HowTheParetoFrontierChangesWithGADuration_2017-1Window},
    \ref{fig:BackTestingTheResultsOfTheGA},
    \ref{fig:GeneralStockDistributionCombinedOverAllWindowsTested}, and
    \ref{fig:HowTheInduvidualPortfolioWasDistributedOverEachWindow} were all hand
    crafted from the results. Whereas building in a direct graphing tool would make
    analysis much easier. It could also be extended to incorporate other assets.
    This would be an easy change to make, but could also have an API to do so
    to make it infinitely extend-able.

\pagebreak
\bibliographystyle{IEEEtran}
\bibliography{Report}

\end{document}

% DEMO
% learning ga myself
% learning portfolio optimization
% https://intrinio.com/

% REPORT
% -- 1 more page on GA about NSGA-2
% -- why use NSGA-2 over scalarizing alteratives

% show the signifance of the project
% show the novelty